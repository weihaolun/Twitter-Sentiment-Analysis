{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "478aa9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sending GET requests from the API\n",
    "import requests\n",
    "# For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "#To add wait time between requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "592f2dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "bearer_token = os.environ.get(\"bearer-token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6dc2f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAAHalVAEAAAAAVViwLGyKNUuT6ssD8tDTqiyQ1Pw%3DVXK9D7qXXABAcb96CPXxbltVZy3C9mzONePX8nLr9mkN3oHa7u'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77052a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth():\n",
    "    return os.environ.get(\"TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca75c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2RecentSearchPython\"\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2970fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5290c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url(keyword, start_date, end_date, next_token = None):\n",
    "    \n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/recent\" #Change to the endpoint you want to collect data from\n",
    "\n",
    "    #change params based on the endpoint you are using\n",
    "    query_params = {'query': keyword, \n",
    "                    'start_time' : start_time, \n",
    "                    'end_time' : end_time,\n",
    "                    'max_results' : max_results,\n",
    "                    'tweet.fields': 'created_at',\n",
    "                    'next_token': {}\n",
    "                     }\n",
    "    \n",
    "    return (search_url, query_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f4d74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_endpoint(url, params, next_token = None):\n",
    "    params['next_token'] = next_token   #params object received from create_url function\n",
    "    response = requests.get( url, auth=bearer_oauth, params = params)\n",
    "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs for tweets\n",
    "#bearer_token = auth()\n",
    "#headers = create_headers(bearer_token)\n",
    "keyword = \"amazon lang:en\"\n",
    "start_list =    ['2021-10-01T00:00:00.000Z', \n",
    "                    '2021-10-02T00:00:00.000Z', \n",
    "                    '2021-10-03T00:00:00.000Z',\n",
    "                    '2021-10-04T00:00:00.000Z',\n",
    "                    '2021-10-05T00:00:00.000Z',\n",
    "                    '2021-10-06T00:00:00.000Z',\n",
    "                    '2021-10-07T00:00:00.000Z',\n",
    "                    '2021-10-08T00:00:00.000Z',\n",
    "                    '2021-10-09T00:00:00.000Z',\n",
    "                    '2021-10-10T00:00:00.000Z',\n",
    "                    '2021-10-11T00:00:00.000Z',\n",
    "                    '2021-10-12T00:00:00.000Z',\n",
    "                    '2021-10-13T00:00:00.000Z',\n",
    "                    '2021-10-14T00:00:00.000Z',\n",
    "                    '2021-10-15T00:00:00.000Z',\n",
    "                    '2021-10-16T00:00:00.000Z',\n",
    "                    '2021-10-17T00:00:00.000Z',\n",
    "                    '2021-10-18T00:00:00.000Z',\n",
    "                    '2021-10-19T00:00:00.000Z',\n",
    "                    '2021-10-20T00:00:00.000Z',\n",
    "                    '2021-10-21T00:00:00.000Z',\n",
    "                    '2021-10-22T00:00:00.000Z',\n",
    "                    '2021-10-23T00:00:00.000Z',\n",
    "                    '2021-10-24T00:00:00.000Z',\n",
    "                    '2021-10-25T00:00:00.000Z',\n",
    "                    '2021-10-26T00:00:00.000Z',\n",
    "                    '2021-10-27T00:00:00.000Z',\n",
    "                    '2021-10-28T00:00:00.000Z',\n",
    "                    '2021-10-29T00:00:00.000Z',\n",
    "                    '2021-10-30T00:00:00.000Z',\n",
    "                    '2021-10-31T00:00:00.000Z'\n",
    "                ]\n",
    "\n",
    "end_list =    ['2021-10-01T23:59:59.000Z',\n",
    "                    '2021-10-02T23:59:59.000Z',\n",
    "                    '2021-10-03T23:59:59.000Z',\n",
    "                    '2021-10-04T23:59:59.000Z',\n",
    "                    '2021-10-05T23:59:59.000Z',\n",
    "                    '2021-10-06T23:59:59.000Z',\n",
    "                    '2021-10-07T23:59:59.000Z',\n",
    "                    '2021-10-08T23:59:59.000Z',\n",
    "                    '2021-10-09T23:59:59.000Z',\n",
    "                    '2021-10-10T23:59:59.000Z',\n",
    "                    '2021-10-11T23:59:59.000Z',\n",
    "                    '2021-10-12T23:59:59.000Z',\n",
    "                    '2021-10-13T23:59:59.000Z',\n",
    "                    '2021-10-14T23:59:59.000Z',\n",
    "                    '2021-10-15T23:59:59.000Z',\n",
    "                    '2021-10-16T23:59:59.000Z',\n",
    "                    '2021-10-17T23:59:59.000Z',\n",
    "                    '2021-10-18T23:59:59.000Z',\n",
    "                    '2021-10-19T23:59:59.000Z',\n",
    "                    '2021-10-20T23:59:59.000Z',\n",
    "                    '2021-10-21T23:59:59.000Z',\n",
    "                    '2021-10-22T23:59:59.000Z',\n",
    "                    '2021-10-23T23:59:59.000Z',\n",
    "                    '2021-10-24T23:59:59.000Z',\n",
    "                    '2021-10-25T23:59:59.000Z',\n",
    "                    '2021-10-26T23:59:59.000Z',\n",
    "                    '2021-10-27T23:59:59.000Z',\n",
    "                    '2021-10-28T23:59:59.000Z',\n",
    "                    '2021-10-29T23:59:59.000Z',\n",
    "                    '2021-10-30T23:59:59.000Z',\n",
    "                    '2021-10-31T23:59:59.000Z'\n",
    "                ]\n",
    "\n",
    "max_results = 540\n",
    "\n",
    "#Total number of tweets we collected from the loop\n",
    "total_tweets = 0\n",
    "\n",
    "# Create file\n",
    "csvFile = open(\"data.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "#Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
    "csvWriter.writerow(['created_at','id','tweet'])\n",
    "csvFile.close()\n",
    "\n",
    "for i in range(0,len(start_list)):\n",
    "\n",
    "    # Inputs\n",
    "    count = 0 # Counting tweets per time period\n",
    "    max_count = 100 # Max tweets per time period\n",
    "    flag = True\n",
    "    next_token = None\n",
    "    \n",
    "    # Check if flag is true\n",
    "    while flag:\n",
    "        # Check if max_count reached\n",
    "        if count >= max_count:\n",
    "            break\n",
    "        print(\"-------------------\")\n",
    "        print(\"Token: \", next_token)\n",
    "        url = create_url(keyword, start_list[i],end_list[i])\n",
    "        json_response = connect_to_endpoint(url[0], url[1], next_token)\n",
    "        result_count = json_response['meta']['result_count']\n",
    "\n",
    "        if 'next_token' in json_response['meta']:\n",
    "            # Save the token to use for next call\n",
    "            next_token = json_response['meta']['next_token']\n",
    "            print(\"Next Token: \", next_token)\n",
    "            if result_count is not None and result_count > 0 and next_token is not None:\n",
    "                print(\"Start Date: \", start_list[i])\n",
    "                append_to_csv(json_response, \"data.csv\")\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(5)                \n",
    "        # If no next token exists\n",
    "        else:\n",
    "            if result_count is not None and result_count > 0:\n",
    "                print(\"-------------------\")\n",
    "                print(\"Start Date: \", start_list[i])\n",
    "                append_to_csv(json_response, \"data.csv\")\n",
    "                count += result_count\n",
    "                total_tweets += result_count\n",
    "                print(\"Total # of Tweets added: \", total_tweets)\n",
    "                print(\"-------------------\")\n",
    "                time.sleep(5)\n",
    "            \n",
    "            #Since this is the final request, turn flag to false to move to the next time period.\n",
    "            flag = False\n",
    "            next_token = None\n",
    "        time.sleep(5)\n",
    "print(\"Total number of results: \", total_tweets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
